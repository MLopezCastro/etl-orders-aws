
# ğŸ› ï¸ ETL de Ã“rdenes de Compra + AnÃ¡lisis en AWS

Este proyecto implementa un pipeline **ETL (Extract, Transform, Load)** sobre un archivo CSV de Ã³rdenes de compra, aplicando limpieza y validaciÃ³n de datos con Python, y luego carga el resultado a **AWS S3** para ser consultado mediante **Amazon Athena**. Es ideal para automatizar reportes o anÃ¡lisis en la nube.

---

## ğŸ“ Estructura del Proyecto

```

ecommerce/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # CSV original sin limpiar
â”‚   â””â”€â”€ output/              # CSV limpio final
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ etl.log              # Log del proceso ETL
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ etl\_functions.py     # Funciones auxiliares de limpieza
â”œâ”€â”€ main.py                  # Script principal del ETL
â”œâ”€â”€ requirements.txt         # Dependencias del proyecto
â””â”€â”€ README.md                # DocumentaciÃ³n del proyecto

````

---

## ğŸš€ CÃ³mo ejecutar el pipeline

Asegurate de tener Python 3.10+ y las dependencias instaladas:

```bash
pip install -r requirements.txt
````

Luego corrÃ© el ETL:

```bash
python main.py --input data/raw/orders.csv --output data/output/orders_clean.csv
```

Esto va a:

1. Leer el CSV crudo.
2. Validar y limpiar los datos.
3. Exportar un nuevo archivo limpio en `data/output/`.
4. Dejar logs detallados del proceso en `logs/etl.log`.

---

## â˜ï¸ Carga en AWS

Una vez generado el CSV limpio (`orders_clean.csv`), se cargÃ³ a **Amazon S3** en la ruta:

```
s3://marcelo-orders-bucket/data/orders_clean.csv
```

Luego, se creÃ³ una tabla externa en **Amazon Athena** apuntando al bucket:

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS default.orders_clean (
  order_id BIGINT,
  order_date DATE,
  customer_id BIGINT,
  product_id BIGINT,
  product_name STRING,
  quantity BIGINT,
  unit_price DOUBLE,
  currency STRING,
  status STRING,
  total_amount DOUBLE
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES ('field.delim'=',')
LOCATION 's3://marcelo-orders-bucket/data/'
TBLPROPERTIES ('skip.header.line.count'='1');
```

---

## ğŸ” AnÃ¡lisis con SQL en Athena

Una vez cargada la tabla, se realizaron consultas para analizar las ventas por estado:

```sql
SELECT status, SUM(total_amount) AS total_sales
FROM orders_clean
GROUP BY status;
```

ğŸ”¹ Resultado:

| status   | total\_sales |
| -------- | ------------ |
| pending  | 49.99        |
| returned | 89.97        |
| shipped  | 39.98        |

---

## ğŸ§° Stack TecnolÃ³gico

* ğŸ **Python 3.10**

  * `pandas` para procesamiento de datos
  * `boto3` para conexiÃ³n a AWS
  * `dateutil` para manejo de fechas
* â˜ï¸ **Amazon S3** (almacenamiento en la nube)
* ğŸ“Š **Amazon Athena** (consultas SQL serverless)

---

## ğŸ“¦ Dependencias

Listado en `requirements.txt`:

```
pandas==2.2.2
boto3==1.34.124
python-dateutil==2.9.0
```

InstalaciÃ³n:

```bash
pip install -r requirements.txt
```

---

## âœ… Resultado Final

* CSV limpio listo para anÃ¡lisis: `data/output/orders_clean.csv`
* Logs detallados del proceso: `logs/etl.log`
* Datos disponibles en S3 para ser consultados con Athena
* SQL en la nube sin necesidad de infraestructura propia

---

## ğŸ§  Autor

Marcelo FabiÃ¡n LÃ³pez â€“ Data Analyst
ğŸ”— [LinkedIn](https://www.linkedin.com/in/marcelolopezcastro) | [GitHub](https://github.com/MLopezCastro)

```
